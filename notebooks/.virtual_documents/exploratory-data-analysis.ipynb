














# Import necessary libraries

import re
import pandas as pd
import numpy as np

# Visualization libraries

import matplotlib.pyplot as plt
import seaborn as sns

# Statistics libraries

import statsmodels as sm
from scipy.stats import f_oneway, chi2_contingency, kendalltau, levene

# Machine Learning libraries

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


import warnings
from sklearn.exceptions import FitFailedWarning

warnings.filterwarnings('ignore', category=FitFailedWarning)
warnings.filterwarnings('once', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)


# Set a random_state number to be used throughout the analysis

random_state = 42


# Import cleaned data

survey_df = pd.read_csv('../data/interim/clean_survey_results.csv')
survey_df.head()





# Visualize overall distribution of BPM

plt.hist(survey_df['BPM'])
plt.xlabel('BPM')
plt.ylabel('# of observations')
plt.title('Distribution of BPM')

plt.show()





# Visualize distribution of BPM by genre

sns.boxplot(x='Fav genre', y='BPM', data=survey_df)

plt.xticks(rotation=90)
plt.title('Distribution of BPM by Favorite Genre')

plt.show()








# Separate data by genre
genres = survey_df['Fav genre'].unique()
bpm_groups = [survey_df[survey_df['Fav genre'] == genre]['BPM'] for genre in genres]

# Perform one-way ANOVA
f_stat, p_value = f_oneway(*bpm_groups)

print(f"F-statistic: {f_stat}, p-value: {p_value}")








# Create Musician column

survey_df['Musician'] = ((survey_df['Instrumentalist'] == 'Yes') | (survey_df['Composer'] == 'Yes')).astype('str')
survey_df.head()


# Calculate proportion of musicians to nonmusicians by music effects in order to plot

counts = survey_df.groupby(['Music effects', 'Musician']).size().reset_index(name='Count')
totals = survey_df.groupby('Music effects')['Musician'].count().reset_index(name='Total')
merged_df = pd.merge(counts, totals)
merged_df['Proportion'] = (merged_df['Count'] / merged_df['Total'])

merged_df


# Plot data

sns.catplot(data=merged_df, x='Music effects', y='Proportion', hue='Musician', kind='bar')

plt.title('Proportion of Music effects by Musician status')

plt.show()





# Create contingency table

contingency_table = pd.crosstab(survey_df['Music effects'], survey_df['Musician'])
print(contingency_table)


# Run chi-squared test using sample contingency table

chi2, p, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-squared test statistic: {chi2}, P-value: {p}")





# Bootstrap data with n=2,000

bootstrapped_survey = pd.concat([survey_df, survey_df.sample(n=2000, replace=True, random_state=random_state)])
bootstrapped_survey.shape


# Calculate proportions in bootstrapped data

counts = bootstrapped_survey.groupby(['Music effects', 'Musician']).size().reset_index(name='Count')
totals = bootstrapped_survey.groupby('Music effects')['Musician'].count().reset_index(name='Total')
merged_df = pd.merge(counts, totals)
merged_df['Proportion'] = (merged_df['Count'] / merged_df['Total'])

merged_df


# Plot data

sns.catplot(data=merged_df, x='Music effects', y='Proportion', hue='Musician', kind='bar')

plt.title('Proportion of Music effects by Musician status')

plt.show()





# Re-create contingency table with bootstrapped sample

contingency_table = pd.crosstab(bootstrapped_survey['Music effects'], bootstrapped_survey['Musician'])
contingency_table


# Run chi-squared test using bootstrapped contingency table

chi2, p, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-squared test statistic: {chi2}, P-value: {p}")








# Create Improve column which represents whether the patient improved or not

survey_df['Improve'] = (survey_df['Music effects'] == 'Improve').astype(str)
survey_df.head()


# Collapse data into three bins: Mild, Moderate, and Severe

cols_to_bin = ['Anxiety', 'Depression', 'Insomnia', 'OCD']
group_names = ['Mild', 'Moderate', 'Severe']
new_col_names = []

for col in cols_to_bin:
    new_name = f"{col}_binned"
    new_col_names.append(new_name)
    survey_df[new_name] = pd.cut(survey_df[col], bins=3, labels=group_names)

survey_df.head()


# Create contingency tables and conduct chi-squared tests

contingency_tables = {}

for col in new_col_names:
    contingency_table = pd.crosstab(survey_df[col], survey_df['Improve'])
    contingency_tables[col] = contingency_table
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    print(f"Chi-squared test statistic for {col}: {round(chi2, 2)}, P-value: {p}")


# Pull up contingency table for Anxiety

contingency_tables['Anxiety_binned']








# Create worsen column

survey_df['Worsen'] = survey_df['Music effects'] == 'Worsen'
survey_df.head()


# Create contingency tables and conduct chi-squared tests

cols = ['Depression', 'Insomnia', 'OCD']
contingency_tables = {}

for col in cols:
    contingency_table = pd.crosstab(survey_df[col], survey_df['Worsen'])
    contingency_tables[col] = contingency_table
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    print(f"Chi-squared test statistic for {col}: {round(chi2, 2)}, P-value: {p}")


# Pull up contingency table for Depression

contingency_tables['Depression']








# Visualize distribution of Age

plt.hist(survey_df['Age'], bins=15)

plt.xlabel('Age')
plt.ylabel('# of observations')
plt.title('Distribution of Age')

plt.show()





# Bin Age column into 5 equally sized bins

bins, bin_edges = pd.qcut(survey_df['Age'], retbins=True, q=5)
labels = [f"{int(bin_edges[i])}-{int(bin_edges[i+1])}" for i in range(len(bin_edges)-1)]
survey_df['Age_Binned'] = pd.qcut(survey_df['Age'], labels=labels, q=5)


# Transform data so we can plot the data based on number of very frequent listeners per age group

frequency_cols = list(survey_df.columns[survey_df.columns.str.startswith('Fr')])

very_frequent_listeners = survey_df[(survey_df[frequency_cols] == 'Very frequently').any(axis=1)]
melted_df = very_frequent_listeners.melt(id_vars=['Age', 'Age_Binned'], value_vars=frequency_cols, var_name='Genre', value_name='Frequency')

# Filter for 
melted_df = melted_df[melted_df['Frequency'] == 'Very frequently']

# Prettify Genre column so it only lists the actual Genre
melted_df['Genre'] = melted_df['Genre'].apply(lambda x: str(re.findall(r'\[(.*?)\]', x)[0]))

melted_df


# Plot strip-plot of very frequent listeners of each genre and group by age groups

plt.figure(figsize=(12, 6))
sns.stripplot(x='Genre', y='Age', data=melted_df)

plt.xticks(rotation=90)
plt.title('Very Frequent Listeners of Music Genres')

plt.show()


# Create contingency table and plot data

contingency_table = pd.crosstab(melted_df['Genre'], melted_df['Age_Binned'], margins=True).sort_values(by='All', ascending=False)
contingency_table = contingency_table.drop('All', axis='index').drop('All', axis=1)

fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15, 8))
contingency_table.plot(kind='bar', ax=ax[0])

ax[0].set_title('Very Frequent Genre Listeners by Age Group')
ax[0].set_xlabel('Genre')
ax[0].set_ylabel('# of Listeners')
ax[0].legend_ = None
fig.legend(title='Age group', bbox_to_anchor=(0.98,0.6))

# Plot normalized data for comparison

normalized_table = pd.crosstab(melted_df['Genre'], melted_df['Age_Binned'], normalize=0).reindex(contingency_table.index)
normalized_table.plot(kind='bar', ax=ax[1])
ax[1].set_ylabel('% of Listeners')
ax[1].legend_ = None

plt.show()








# Visualize distributions to determine whether lineplot should use median or mean

conditions = cols_to_bin

fig, ax = plt.subplots(2,2, sharex=True, sharey=True)

# Anxiety
ax[0][0].hist(survey_df[conditions[0]])
ax[0][0].set_title(conditions[0])

# Depression
ax[0][1].hist(survey_df[conditions[1]])
ax[0][1].set_title(conditions[1])

# Insomnia
ax[1][0].hist(survey_df[conditions[2]])
ax[1][0].set_title(conditions[2])

# OCD
ax[1][1].hist(survey_df[conditions[3]])
ax[1][1].set_title(conditions[3])

ax[1][0].set_xlabel('Rating')
ax[1][0].set_ylabel('# of observations')
ax[1][1].set_xlabel('Rating')
ax[0][0].set_ylabel('# of observations')

plt.show()


# Visualize relationship with line plots

fig, ax = plt.subplots(1, 2, sharey=True, figsize=(12,6))

for i, condition in enumerate(conditions):
    sns.pointplot(data=survey_df, x='Age_Binned', y=condition, ax=ax[0], color=sns.color_palette()[i], label=condition)
    sns.pointplot(data=survey_df, x='Age_Binned', y=condition, ax=ax[1], color=sns.color_palette()[i], label=condition, estimator='median')

ax[0].set_xlabel('Age Group')
ax[1].set_xlabel('Age Group')
ax[0].set_ylabel('Mean Disorder Rating')
ax[1].set_ylabel('Median Disorder Rating')
ax[0].set_title('Mean Disorder Scoring')
ax[1].set_title('Median Disorder Scoring')
plt.legend(bbox_to_anchor=(1,1))

plt.show()








# Visualize relationship between age and hours per day

sns.boxplot(x='Age_Binned', y='Hours per day', data=survey_df[['Age_Binned', 'Hours per day']])

plt.xlabel('Age Groups')
plt.title('Hours per Day per Age Group')

plt.show()








# Conduct ANOVA test

# Insomnia_binned ranks
ranks = survey_df['Insomnia_binned'].unique()
hours_groups = [survey_df[survey_df['Insomnia_binned'] == rank]['Hours per day'] for rank in ranks]

# Perform one-way ANOVA
f_stat, p_value = f_oneway(*hours_groups)

print(f"F-statistic: {f_stat}, p-value: {p_value}")





# Perform kendall-tau test

tau, p_value = kendalltau(survey_df['Hours per day'], survey_df['Insomnia_binned'])

print(f"Tau-value: {round(tau, 2)}, p_value: {p_value}")








# Define method which returns True if Fav genre's matching Frequency column has the value 'Very frequently'

def fav_frequent(row):
    favorite_genre = row['Fav genre']
    col_name = f"Frequency [{favorite_genre}]"
    return row[col_name] == 'Very frequently'


# Apply fav_frequent method

survey_df['Fav_frequent'] = survey_df.apply(fav_frequent, axis=1)

# Verify results

survey_df[['Fav genre', *frequency_cols, 'Fav_frequent']]


# Look at proportion of patients with matching fav genre and frequency

survey_df['Fav_frequent'].value_counts(normalize=True)








# Import ml preprocessing data for this since encoding already done there

encoded_df = pd.read_csv('./../data/processed/pre_processed_survey_results.csv')
encoded_df.head()


# Create new column called frequency_var which stores the variance across Frequency columns

encoded_df['frequency_var'] = encoded_df[frequency_cols].var(axis=1)
encoded_df.head()


# Compare mean variance for both groups

encoded_df.groupby('Exploratory')['frequency_var'].mean()





# Perform levene's test

exploratory_group = encoded_df[encoded_df['Exploratory'] == 1][frequency_cols].to_numpy().flatten()
non_exploratory_group = encoded_df[encoded_df['Exploratory'] == 0][frequency_cols].to_numpy().flatten()

stat, p_value = levene(exploratory_group, non_exploratory_group)

print(f"Levene's statistic: {stat}, p_value: {p_value}")





# Visualize distribution of variance

sns.histplot(encoded_df['frequency_var'], bins=20, kde=True)

plt.title('Variance Across Frequency [Genre] Columns')
plt.xlabel('Variance')

plt.show()





# Load preprocessed data

ml_survey_df = pd.read_csv('../data/processed/pre_processed_survey_results.csv')
ml_survey_df.head()





# Convert While working, Instrumentalist, Composer, Fav genre, Exploratory, Anxiety, Depression, Insomnia, OCD, Music effects, and Foreign languages to type category.

columns_to_convert = ['While working', 'Instrumentalist', 'Composer', 'Exploratory', 'Foreign languages', 'Anxiety', 'Depression', 'Insomnia', 'OCD']

ml_survey_df[columns_to_convert] = ml_survey_df[columns_to_convert].astype('category')


# Convert Frequency columns into category

frequency_columns = list(ml_survey_df.columns[ml_survey_df.columns.str.startswith('Fr')])
ml_survey_df[frequency_columns] = ml_survey_df[frequency_columns].astype('category')

ml_survey_df[frequency_columns]


# Confirm all conversions

ml_survey_df.dtypes





# Scale Age, Hours per day, and BPM

cols_to_scale = ['Age', 'Hours per day', 'BPM']

# Initialize Scaler 

scaler = StandardScaler()

# Fit scaler

ml_survey_df[cols_to_scale] = scaler.fit_transform(ml_survey_df[cols_to_scale])
ml_survey_df


ml_survey_df.columns





# Separate data in dependent and explanatory variables

X = ml_survey_df.drop(['Improve', 'No effect', 'Worsen'], axis=1)
y = ml_survey_df['Improve']

# Split data in training and testing data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=random_state)

# Set up cross-validation and logistic regression

kf = KFold(shuffle=True, random_state=random_state)
lr = LogisticRegression(max_iter=1000)

# Run model and print summarized results

cv_results = cross_val_score(lr, X_train, y_train, cv=kf)
print(f"Standard deviation of accuracy scores: {round(np.std(cv_results), 2)}")
print(f"Mean accuracy score: {round(np.mean(cv_results), 2)}")





# Calculate no-information rate for Improve

print(f"No-Information rate for True predictions: {round(ml_survey_df['Improve'].value_counts(normalize=True)[1], 2)}")





# Set up hyperparameter tuning with a grid search

param_grid = {
    'penalty' : ['elasticnet', 'l2', 'l1'],
    'C' : np.linspace(0, 4),
    'solver' : ['lbfgs', 'liblinear', 'saga']
}

lr = LogisticRegression(max_iter=3000)
model_cv = GridSearchCV(lr, param_grid, cv=kf)
model_cv.fit(X_train, y_train)

# Print results

print(f"Mean accuracy score: {round(model_cv.best_score_, 2)}")
print(f"Best parameters: {model_cv.best_params_}")





# Fit model with best parameters to hidden test data

predictions = model_cv.predict(X_test)

# Visualize performance with classification report

print(classification_report(y_test, predictions))


# Create dataframe of coefficients and plots visualization

coefficient_df = pd.DataFrame({'Feature':X.columns, 'Coefficient':model_cv.best_estimator_.coef_[0]}).sort_values(by='Coefficient', ascending=False).reset_index(drop=True)
relevant_coef = coefficient_df[coefficient_df['Coefficient'] != 0]

plt.barh(relevant_coef['Feature'], relevant_coef['Coefficient'])

plt.title('Logistic Regression Coefficients')
plt.xlabel('Coefficient')
plt.ylabel('Feature')

plt.show()
